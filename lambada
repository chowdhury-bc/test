import boto3
from os import getenv
from opensearchpy import OpenSearch, RequestsHttpConnection, exceptions
from requests_aws4auth import AWS4Auth
from io import BytesIO
import requests
import json
from decimal import Decimal
import logging
import re
import base64

from agents.retriever_agent import fetch_data, fetch_data_v2, classify_and_translation_request
from prompt_utils import AGENT_MAP, get_system_prompt, agent_execution_step, rag_chat_bot_prompt
from prompt_utils import casual_prompt, get_classification_prompt, RESERVED_TAGS
from prompt_utils import get_can_the_orchestrator_answer_prompt
from prompt_utils import sentiment_prompt, generate_claude_3_ocr_prompt
from prompt_utils import pii_redact_prompt
from agent_executor_utils import agent_executor
from pypdf import PdfReader

bedrock_client = boto3.client('bedrock-runtime')
embed_model_id = getenv("EMBED_MODEL_ID", "amazon.titan-embed-image-v1")
LOG = logging.getLogger()
LOG.setLevel(logging.INFO)
endpoint = getenv("OPENSEARCH_VECTOR_ENDPOINT", "https://admin:P@@dummy-amazonaws.com:443")

SAMPLE_DATA_DIR = getenv("SAMPLE_DATA_DIR", "/var/task")
INDEX_NAME = getenv("VECTOR_INDEX_NAME", "sample-embeddings-store-dev")
wss_url = getenv("WSS_URL", "WEBSOCKET_URL_MISSING")
rest_api_url = getenv("REST_ENDPOINT_URL", "REST_URL_MISSING")
is_rag_enabled = getenv("IS_RAG_ENABLED", 'yes')
s3_bucket_name = getenv("S3_BUCKET_NAME", "S3_BUCKET_NAME_MISSING")
websocket_client = boto3.client('apigatewaymanagementapi', endpoint_url=wss_url)
lambda_client = boto3.client('lambda')

credentials = boto3.Session().get_credentials()
service = 'aoss'
region = getenv("REGION", "us-east-1")
awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,
                   region, service, session_token=credentials.token)

def query_rag_no_agent(user_input, query_vector_db, language, model_id, is_hybrid_search, connect_id):
    global rag_chat_bot_prompt
    final_prompt = rag_chat_bot_prompt
    chat_input = json.loads(user_input)
    LOG.debug(f'Chat history {chat_input}')
    can_invoke_model = False

    user_chat_history = ''
    for chat in chat_input:
        if 'role' in chat and chat['role'] == 'user':
            for message in chat['content']:
                if message['type'] == 'text':
                    user_conv_wo_context = re.sub('<context>.*?</context>','Context redacted',message['text'], flags=re.DOTALL)
                    user_chat_history += 'user: ' + user_conv_wo_context + '. '
                elif message['type'] == 'image' and 'source' in message and 'partial_s3_key' in message['source']:
                    s3_key = f"bedrock/data/{message['source']['partial_s3_key']}"
                    del message['source']
                    message['type']='text'
                    message['text'] = f"content at S3 location: {s3_key}"
                    user_chat_history += 'user:' + message['text']
        elif 'role' in chat and chat['role'] == 'assistant':
            for message in chat['content']:
                if message['type'] == 'text':
                    user_chat_history += 'assistant: ' + message['text'] + '. '
    # First step is classification
    context = None
    classify_translate_json = classify_and_translation_request(user_chat_history)

    # MODIFIED: Always use vector database for retrieval queries, regardless of query_vector_db parameter
    if 'QUERY_TYPE' in classify_translate_json and classify_translate_json['QUERY_TYPE'] == 'RETRIEVAL':
        if 'TRANSLATED_QUERY' in classify_translate_json:
            reformulated_q = classify_translate_json['TRANSLATED_QUERY']
            proper_nouns = []
            if 'PROPER_NOUNS' in classify_translate_json:
                proper_nouns = classify_translate_json['PROPER_NOUNS']
            # Use vector search regardless of query_vector_db parameter
            context = fetch_data_v2(reformulated_q, proper_nouns, is_hybrid_search)
    
    if 'role' in chat_input[-1] and 'user' == chat_input[-1]['role']:
        can_invoke_model=True
        for text_inputs in chat_input[-1]['content']:
            if text_inputs['type'] == 'text' and '<user-question>' not in text_inputs['text']:
                text_inputs['text'] = f'<user-question> {text_inputs["text"]} </user-question>'
                if 'QUERY_TYPE' in classify_translate_json and classify_translate_json['QUERY_TYPE'] == 'RETRIEVAL' and context is not None:
                    text_inputs['text'] = f"""<context> {context} </context> {text_inputs['text']} """
                elif 'QUERY_TYPE' in classify_translate_json and classify_translate_json['QUERY_TYPE'] == 'CASUAL':
                    final_prompt = rag_chat_bot_prompt + casual_prompt
                break
            elif text_inputs['type'] == 'image':
                if 'source' in text_inputs and 'partial_s3_key' in text_inputs['source']:
                    s3_key = f"bedrock/data/{text_inputs['source']['partial_s3_key']}"
                    LOG.debug(f'Fetch document from S3 {s3_key}')
                    encoded_file = base64.b64encode(get_file_from_s3(s3_bucket_name, s3_key))
                    del text_inputs['source']['partial_s3_key']
                    del text_inputs['source']['file_extension']
                    text_inputs['source']['data'] = encoded_file.decode('utf-8')

    if can_invoke_model:
        prompt_template = {
                        "anthropic_version": "bedrock-2023-05-31",
                        "max_tokens": 70000,
                        "system": final_prompt + f'. You will always respond in {language} language',
                        "messages": chat_input
        }

        LOG.info(f'chat prompt_template {prompt_template}')
        invoke_model(0, prompt_template, connect_id, True, model_id)

def handler(event, context):
    global region
    global websocket_client
    LOG.info(
        "---  Amazon Opensearch Serverless vector db example with Amazon Bedrock Models ---")
    LOG.info(f'event - {event}')

    if 'httpMethod' not in event and 'requestContext' in event:
    # this is a websocket request
        stage = event['requestContext']['stage']
        api_id = event['requestContext']['apiId']
        domain = f'{api_id}.execute-api.{region}.amazonaws.com'
        websocket_client = boto3.client('apigatewaymanagementapi', endpoint_url=f'https://{domain}/{stage}')

        connect_id = event['requestContext']['connectionId']
        routeKey = event['requestContext']['routeKey']

        if routeKey != '$connect':
            if 'body' in event:
                input_to_llm = json.loads(event['body'], strict=False)
                LOG.info('input_to_llm: ', input_to_llm)
                query = input_to_llm['query']
                language = 'english'
                if 'language' in input_to_llm:
                    language = input_to_llm['language']
                behaviour = input_to_llm['behaviour']
                if behaviour == 'advanced-agent':
                    query_agents(behaviour, query, connect_id)
                elif behaviour == 'sentiment':
                    model_id = input_to_llm['model_id']
                    query_sentiment(query, model_id, connect_id)
                elif behaviour == 'ocr':
                    model_id = input_to_llm['model_id']
                    perform_ocr(query, model_id, connect_id)
                elif behaviour == 'pii':
                    model_id = input_to_llm['model_id']
                    pii_redact(query, model_id, connect_id)
                else:
                    # Even though we pass query_vector_db parameter, the function will always use the vector database
                    query_vector_db = 'no'
                    if 'query_vectordb' in input_to_llm and input_to_llm['query_vectordb']=='yes':
                        query_vector_db='yes' 
                    if 'model_id' in input_to_llm:
                        model_id = input_to_llm['model_id']
                    is_hybrid_search = False
                    if 'is_hybrid_search' in input_to_llm and input_to_llm['is_hybrid_search'] == 'yes':
                        is_hybrid_search = True
                    query_rag_no_agent(query, query_vector_db, language, model_id, is_hybrid_search, connect_id)
        elif routeKey == '$connect':
            # TODO Add authentication of access token
            if 'access_token' in event['queryStringParameters']:
                headers = {'Content-Type': 'application/json', 'Authorization': event['queryStringParameters']['access_token'] }
                response = requests.get(f'{rest_api_url}connect-tracker', headers=headers, verify=False)
                if response.status_code == 200:
                    return {'statusCode': '200', 'body': 'Bedrock says hello' }
                else:
                    LOG.error(f'Response Error status_code: {response.status_code}, reason: {response.reason}')
                    return {'statusCode': f'{response.status_code}', 'body': f'Forbidden, {response.reason}' }
            return {'statusCode': 400, 'body': f'Forbidden, cant establish secure socket connection' }
                
    # Rest of the handler function remains unchanged
    return {'statusCode': '200', 'body': 'Bedrock says hello' }

# Include required helper functions
def invoke_model(step_id, prompt, connect_id, send_on_socket=False, model_id = "anthropic.claude-3-sonnet-20240229-v1:0"):
    result = query_bedrock_claude3_model(step_id, model_id, prompt, connect_id, send_on_socket)
    return ''.join(result)

def query_bedrock_claude3_model(step_id, model, prompt, connect_id, send_on_socket=False):
    '''
       StepId and ConnectId can be used to stream data over the  socket
    '''
    cnk_str = []
    response = bedrock_client.invoke_model_with_response_stream(
        body=json.dumps(prompt),
        modelId=model,
        accept='application/json',
        contentType='application/json'
    )
    counter=0
    sent_ack = False
    for evt in response['body']:
        counter = counter + 1
        if 'chunk' in evt:
            chunk = evt['chunk']['bytes']
            chunk_json = json.loads(chunk.decode("UTF-8"))

            if chunk_json['type'] == 'content_block_delta' and chunk_json['delta']['type'] == 'text_delta':
                cnk_str.append(chunk_json['delta']['text'])
                if chunk_json['delta']['text'] and len((chunk_json['delta']['text']).split()) > 0:
                    if send_on_socket:
                        websocket_send(connect_id, { "text": chunk_json['delta']['text'] } )
        else:
            cnk_str.append(evt)
            break
        
        if 'internalServerException' in evt:
            result = evt['internalServerException']['message']
            websocket_send(connect_id, { "text": result } )
            break
        elif 'modelStreamErrorException' in evt:
            result = evt['modelStreamErrorException']['message']
            websocket_send(connect_id, { "text": result } )
            break
        elif 'throttlingException' in evt:
            result = evt['throttlingException']['message']
            websocket_send(connect_id, { "text": result } )
            break
        elif 'validationException' in evt:
            result = evt['validationException']['message']
            websocket_send(connect_id, { "text": result } )
            break

    if send_on_socket:
        websocket_send(connect_id, { "text": "ack-end-of-msg" } )

    return cnk_str

def websocket_send(connect_id, message):
    global websocket_client
    global wss_url
    LOG.debug(f'WSS URL {wss_url}, connect_id {connect_id}, message {message}')
    response = websocket_client.post_to_connection(
                Data=base64.b64encode(json.dumps(message, indent=4).encode('utf-8')),
                ConnectionId=connect_id
            )

def get_file_from_s3(s3bucket, key):
    s3 = boto3.resource('s3')
    obj = s3.Object(s3bucket, key)
    file_bytes = obj.get()['Body'].read()
    LOG.debug(f'returns S3 encoded object from key {s3bucket}/{key}')
    return file_bytes
