#!/bin/bash

# Set up logging
LOGFILE="/usr/bin/code/script_execution.log"
exec > >(tee -a "$LOGFILE") 2>&1
echo "===== Script started at $(date) ====="
echo "Current user: $(whoami)"
echo "Current dir: $(pwd)"
echo "PATH: $PATH"

# Activate the virtual environment properly
VENV_PATH="/usr/bin/code/py311"
echo "Activating virtual environment at $VENV_PATH"
source "${VENV_PATH}/bin/activate"

# Check if virtual environment was activated properly
if [ -z "$VIRTUAL_ENV" ]; then
    echo "Error: Virtual environment not activated properly"
    exit 1
else
    echo "Virtual environment activated successfully: $VIRTUAL_ENV"
    echo "Python executable: $(which python)"
    echo "Python version: $(python --version)"
fi

# Configuration variables with absolute paths
BUCKET_NAME="dataimages"
PREFIX="s3preixforfiles"
DOWNLOAD_DIR="/usr/bin/code/100files"
LAST_TIMESTAMP_FILE="/usr/bin/code/last_processed_timestamp.txt"
PREV_FILE_HASH="/usr/bin/code/previous_file_hash.txt"
PREDICTIONS_FILE="/usr/bin/code/testpredictions/predictions.csv"
INFERENCE_OUTPUT="/usr/bin/code/inference_predictions.csv"
TEMP_FILE="/usr/bin/code/tmp/temp_predictions.csv"
DEBUG_DIR="/usr/bin/code/debug"

# Create debug directory
mkdir -p "$DEBUG_DIR"

# Function to convert UTC to EST timestamp
convert_to_est() {
    local utc_time="$1"
    TZ="America/New_York" date -d "$utc_time" "+%Y-%m-%d %H:%M:%S %Z"
}

# Function to rotate predictions file
manage_predictions_file() {
    local max_lines=1000  # Keep last 1000 predictions (adjust as needed)

    if [ -f "$PREDICTIONS_FILE" ]; then
        # Get the header line
        head -n 1 "$PREDICTIONS_FILE" > "$TEMP_FILE"

        # Append the last max_lines-1 (excluding header) to temp file
        tail -n "$max_lines" "$PREDICTIONS_FILE" >> "$TEMP_FILE"

        # Replace original with rotated file
        mv "$TEMP_FILE" "$PREDICTIONS_FILE"

        # Log the rotation
        echo "Predictions file rotated at $(TZ="America/New_York" date "+%Y-%m-%d %H:%M:%S %Z")"
        echo "Current predictions file size: $(wc -l < "$PREDICTIONS_FILE") lines"
    else
        echo "Warning: Predictions file does not exist at $PREDICTIONS_FILE"
    fi
}

# Function to archive old predictions
archive_predictions() {
    local archive_dir="/usr/bin/code/testpredictions/archives"
    local current_date=$(date +"%Y%m")

    # Create archives directory if it doesn't exist
    mkdir -p "$archive_dir"

    # If it's the first day of the month and predictions file exists
    if [ "$(date +%d)" = "01" ] && [ -f "$PREDICTIONS_FILE" ]; then
        # Create monthly archive
        local archive_file="$archive_dir/predictions_${current_date}.csv"
        if [ ! -f "$archive_file" ]; then
            cp "$PREDICTIONS_FILE" "$archive_file"
            # Compress the archive
            gzip "$archive_file"
            # Upload archive to S3
            /usr/local/bin/aws s3 cp "${archive_file}.gz" "s3://rise-collab/archives/"
            echo "Monthly archive created: ${archive_file}.gz"
            # Clear the predictions file but keep the header
            head -n 1 "$PREDICTIONS_FILE" > "${PREDICTIONS_FILE}.tmp"
            mv "${PREDICTIONS_FILE}.tmp" "$PREDICTIONS_FILE"
        fi
    fi
}

# Clear AWS CLI cache if it exists
if [ -d ~/.aws/cli/cache ]; then
    echo "Clearing AWS CLI cache..."
    rm -rf ~/.aws/cli/cache/*
fi

# Check AWS credentials and configuration
echo "Checking AWS configuration..."
/usr/local/bin/aws configure list

# Check that $DOWNLOAD_DIR is not empty and it points to a valid directory
if [ -z "$DOWNLOAD_DIR" ]; then
    echo "Error: DOWNLOAD_DIR is not set."
    exit 1
elif [ ! -d "$DOWNLOAD_DIR" ]; then
    echo "DOWNLOAD_DIR does not exist. Creating the directory..."
    mkdir -p "$DOWNLOAD_DIR"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to create DOWNLOAD_DIR."
        exit 1
    fi
fi

# Check that temp directory exists
if [ ! -d "$(dirname "$TEMP_FILE")" ]; then
    echo "Creating temp directory..."
    mkdir -p "$(dirname "$TEMP_FILE")"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to create temp directory."
        exit 1
    fi
fi

# Clean up download directory by removing all files
echo "Cleaning download directory: $DOWNLOAD_DIR"
rm -rf "$DOWNLOAD_DIR"/*
if [ $? -ne 0 ]; then
    echo "Error: Failed to clean up DOWNLOAD_DIR."
    exit 1
fi

# Current timestamp for cache busting
TIMESTAMP=$(date +%s)
echo "Using cache-busting timestamp: $TIMESTAMP"

# Get list of all objects with their LastModified timestamps
echo "Fetching S3 objects..."
all_objects=$(/usr/local/bin/aws s3api list-objects-v2 \
    --bucket $BUCKET_NAME \
    --prefix $PREFIX \
    --query "Contents[].{Key: Key, LastModified: LastModified}" \
    --output json)

# Save objects for debugging
echo "$all_objects" > "$DEBUG_DIR/s3_objects_$TIMESTAMP.json"

# Check if we got a valid response
if [ -z "$all_objects" ] || [ "$all_objects" == "null" ]; then
    echo "Error: Failed to get objects from S3 or no objects found."
    exit 1
fi

# Print all objects sorted by timestamp (for debugging)
echo "All objects in bucket (sorted by modification time, showing last 5):"
echo "$all_objects" | jq -r 'sort_by(.LastModified) | .[] | "\(.LastModified) \(.Key)"' | tail -5

# Get the most recent file (sorting by LastModified timestamp)
latest_file_info=$(echo "$all_objects" | jq -r 'sort_by(.LastModified)[-1]')

# Extract the key and last modified timestamp
key=$(echo "$latest_file_info" | jq -r '.Key')
s3_timestamp_utc=$(echo "$latest_file_info" | jq -r '.LastModified')

# Double-check with direct HEAD request
echo "Verifying latest file metadata directly..."
direct_check=$(/usr/local/bin/aws s3api head-object \
    --bucket $BUCKET_NAME \
    --key "$key")

direct_timestamp=$(echo "$direct_check" | jq -r '.LastModified')
echo "Direct check timestamp: $direct_timestamp"
echo "$direct_check" > "$DEBUG_DIR/head_object_$TIMESTAMP.json"

# Check if the key was retrieved and is valid
if [ -z "$key" ] || [ "$key" == "null" ]; then
    echo "Error: No valid key found."
    exit 1
fi

# Convert timestamps to EST
s3_timestamp_est=$(convert_to_est "$s3_timestamp_utc")
current_time_est=$(TZ="America/New_York" date "+%Y-%m-%d %H:%M:%S %Z")

echo "============== Latest File Information =============="
echo "File: $key"
echo "Upload Time (UTC): $s3_timestamp_utc"
echo "Upload Time (EST): $s3_timestamp_est"
echo "Current Time (EST): $current_time_est"
echo "==============================================="

# Check if the file has already been processed
if [ -f "$LAST_TIMESTAMP_FILE" ]; then
    last_processed=$(cat "$LAST_TIMESTAMP_FILE")
    echo "Last processed timestamp: $last_processed"
    
    # Convert to epoch for reliable comparison
    last_epoch=$(date -d "$last_processed" +%s)
    current_epoch=$(date -d "$s3_timestamp_utc" +%s)
    
    echo "Last processed (epoch): $last_epoch"
    echo "Current file (epoch): $current_epoch"
    
    if [ $current_epoch -le $last_epoch ]; then
        echo "CRITICAL: No newer files found. Current file is same or older than last processed."
        
        # Explicitly check for new files
        echo "Checking explicitly for newer files..."
        newer_objects=$(/usr/local/bin/aws s3api list-objects-v2 \
            --bucket $BUCKET_NAME \
            --prefix $PREFIX \
            --query "Contents[?LastModified>'$last_processed'].{Key: Key, LastModified: LastModified}" \
            --output json)
            
        newer_count=$(echo "$newer_objects" | jq 'length')
        echo "$newer_objects" > "$DEBUG_DIR/newer_objects_$TIMESTAMP.json"
        
        if [ "$newer_count" -eq 0 ] || [ "$newer_objects" == "null" ]; then
            echo "No newer files confirmed. Checking if FORCE_PROCESSING is enabled..."
            
            # Only continue if forced or if it's been more than 1 hour since last run
            hours_since=$((($TIMESTAMP - $last_epoch) / 3600))
            echo "Hours since last run: $hours_since"
            
            if [ "$FORCE_PROCESSING" = "1" ] || [ $hours_since -ge 1 ]; then
                if [ "$FORCE_PROCESSING" = "1" ]; then
                    echo "FORCE_PROCESSING is enabled. Continuing despite no new files..."
                else
                    echo "Over 1 hour since last run. Processing anyway as a periodic check..."
                fi
            else
                echo "To force processing anyway, run with: FORCE_PROCESSING=1 $0"
                echo "Exiting due to no new files."
                exit 0
            fi
        else
            echo "Found $newer_count newer files since last run. Continuing..."
            # Use the newest file from the newer objects
            latest_new_file=$(echo "$newer_objects" | jq -r 'sort_by(.LastModified)[-1]')
            key=$(echo "$latest_new_file" | jq -r '.Key')
            s3_timestamp_utc=$(echo "$latest_new_file" | jq -r '.LastModified')
            echo "Selected newest file: $key (Last modified: $s3_timestamp_utc)"
        fi
    else
        echo "New file detected! Timestamp is newer than last processed."
    fi
else
    echo "No last processed timestamp found. This appears to be the first run."
fi

# Download the latest file to the download directory
echo "Downloading latest file: $key"
/usr/local/bin/aws s3 cp "s3://$BUCKET_NAME/$key" "$DOWNLOAD_DIR/"
if [ $? -ne 0 ]; then
    echo "Error: Failed to download the latest file from S3."
    exit 1
fi

# Get the downloaded file path
downloaded_file="$DOWNLOAD_DIR/$(basename "$key")"

# Check if file was actually downloaded
if [ ! -f "$downloaded_file" ]; then
    echo "Error: Downloaded file not found at $downloaded_file"
    exit 1
fi

# Calculate hash of the downloaded file for comparison
current_file_hash=$(md5sum "$downloaded_file" | awk '{print $1}')
echo "Current file hash: $current_file_hash"

# Check if the file is identical to the previously processed file
if [ -f "$PREV_FILE_HASH" ]; then
    previous_hash=$(cat "$PREV_FILE_HASH")
    echo "Previous file hash: $previous_hash"
    
    if [ "$previous_hash" = "$current_file_hash" ]; then
        echo "Warning: Downloaded file is identical to previously processed file."
        echo "This might indicate an issue with S3 updates or the cron job timing."
    else
        echo "File content has changed from previous run."
    fi
fi

# Save current hash for next run
echo "$current_file_hash" > "$PREV_FILE_HASH"

# Verify only one file was downloaded
file_count=$(ls -1 "$DOWNLOAD_DIR" | wc -l)
echo "Number of files in download directory: $file_count"
if [ "$file_count" -ne 1 ]; then
    echo "Warning: Unexpected number of files in download directory"
    ls -la "$DOWNLOAD_DIR"
    exit 1
fi

# Create a timestamp log
timestamp_log="$DOWNLOAD_DIR/timestamp_log.txt"
echo "File: $(basename $key)" > "$timestamp_log"
echo "Upload Time (EST): $s3_timestamp_est" >> "$timestamp_log"
echo "Processing Time (EST): $current_time_est" >> "$timestamp_log"

# Log the downloaded file info
echo "$downloaded_file $s3_timestamp_est $current_time_est" > /usr/bin/code/files_to_download.txt

# Run the Python script for inference
echo "Starting inference with Python: $(python --version)"
python /usr/bin/code/infer.py --input_img_size 600 \
    --model_path /usr/bin/code/model/* \
    --image_path "$DOWNLOAD_DIR"

# Check if the inference script ran successfully
if [ $? -ne 0 ]; then
    echo "Error: Python inference script failed."
    exit 1
fi

# Check if inference output file exists
if [ ! -f "$INFERENCE_OUTPUT" ]; then
    echo "Error: Inference output file not found at $INFERENCE_OUTPUT"
    exit 1
fi

# Log completion time
completion_time_est=$(TZ="America/New_York" date "+%Y-%m-%d %H:%M:%S %Z")
echo "Inference Completion Time (EST): $completion_time_est" >> "$timestamp_log"

# Make sure predictions directory exists
predictions_dir=$(dirname "$PREDICTIONS_FILE")
if [ ! -d "$predictions_dir" ]; then
    echo "Creating predictions directory..."
    mkdir -p "$predictions_dir"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to create predictions directory."
        exit 1
    fi
fi

# Create predictions file with header if it doesn't exist
if [ ! -f "$PREDICTIONS_FILE" ]; then
    echo "Creating new predictions file..."
    # Copy header from inference output
    head -n 1 "$INFERENCE_OUTPUT" > "$PREDICTIONS_FILE"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to create predictions file."
        exit 1
    fi
fi

# Manage predictions file before appending new data
manage_predictions_file

# Append new predictions to predictions.csv
echo "Appending new predictions to $PREDICTIONS_FILE"
if [ -f "$INFERENCE_OUTPUT" ]; then
    # Count lines before and after
    before_count=$(wc -l < "$PREDICTIONS_FILE")
    
    # Append data (skipping header)
    tail -n +2 "$INFERENCE_OUTPUT" >> "$PREDICTIONS_FILE"
    
    # Count lines after append
    after_count=$(wc -l < "$PREDICTIONS_FILE")
    echo "Added $(($after_count - $before_count)) new prediction lines"
else
    echo "Warning: inference_predictions.csv not found at $INFERENCE_OUTPUT"
fi

# Archive predictions if needed
archive_predictions

# Copy predictions to s3
echo "Uploading predictions to S3..."
/usr/local/bin/aws s3 cp "$PREDICTIONS_FILE" s3://rise-collab/
if [ $? -ne 0 ]; then
    echo "Error: Failed to upload predictions to S3."
    exit 1
fi

# Log file sizes
echo "============== File Size Information =============="
echo "Current predictions file size: $(wc -l < "$PREDICTIONS_FILE") lines"
echo "Current predictions file size (bytes): $(ls -lh "$PREDICTIONS_FILE" | awk '{print $5}')"
echo "==============================================="

# CRITICAL: Save the timestamp AFTER successful processing
echo "$s3_timestamp_utc" > "$LAST_TIMESTAMP_FILE"
echo "Updated last processed timestamp to: $s3_timestamp_utc"

echo "============== Process Complete =============="
echo "Final completion time (EST): $completion_time_est"
echo "Processed file: $key"
echo "==========================================="

# Check if everything worked
if [ $? -eq 0 ]; then
    echo "Script executed successfully at: $completion_time_est (EST)"
    # Clean up downloaded files if needed
    rm -rf "$DOWNLOAD_DIR"/*
else
    echo "Script encountered an error at: $completion_time_est (EST)"
    exit 1
fi

# Deactivate virtual environment
deactivate


* * * * * cd /usr/bin/code && /usr/bin/code/script.sh >> /usr/bin/code/cron.log 2>&1
